# 李宏毅深度学习课程笔记

## 机器学习简介

机器学习就是让机器具有寻找函数的能力

机器学习的任务：回归、分类、结构化学习(Structure Learning)

机器学习寻找函数（训练）一般需要三个步骤：

1. 写出带有未知参数的函数

2. 定义损失函数

   损失函数是未知参数的函数，能够评价参数的好坏

3. 最优化

   梯度下降：选取初始点，计算偏导数，根据学习率计算下一个点

## 深度学习简介

任意非线性函数都可以用分段线性函数来逼近，任意分段线性函数都可以用若干Hard Sigmoid来表示，Hard Sigmoid可以用Soft Sigmoid来近似

$y = c·{1 \over 1+e^{-(b+wx)}} = c·sigmoid(b+wx)$ 

神经元的个数反映了逼近非线性函数所使用的Sigmoid函数的个数

sigmoid可以换成ReLU，这两个是最常用的激活函数

Deep = Many hidden layers

- AlexNet (2012) - 8 layers
- VGG (2014) - 19 layers
- GoogleNet (2014) - 22 layers
- ResidualNet (2015) - 152 layers

为什么选择Deep，不选择Fat？

1. 实验表明，Deep Network的性能比Shallow Network好
2. 理论上说只需一层隐藏层就能产生任何非线性函数，但是产生同样的函数，Deep Network参数量比较少，Shallow Network参数量比较大

## 反向传播(Back Propagation)

反向传播是一个能够有效率地计算高维梯度的算法，是为梯度下降服务的

## 正则化

在损失函数上加上$λ∑(w_{i})^2$，让训练出来的参数尽可能小，从而使得函数更加平滑，对输入不敏感

正则项$λ$如何确定？画出training error和testing error随$λ$的变化曲线

## 回归

1. 确定一个函数集

   $f_{w,b}(x)=∑w_ix_i+b$

2. 评价函数的好坏

   $L(f)={1 \over 2}∑(f(x^n)-ŷ^n)^2$

3. 寻找最好的函数

   梯度下降 $w_i←w_i-η∑-(ŷ-f_{w,b}(x^n))x^n_i$

## 分类

生成模型：贝叶斯算法

步骤：

1. 确定一个函数集

   贝叶斯全概率公式

2. 评价参数的好坏

   使得产生训练数据的概率最大化的参数（极大似然估计、平均值和协方差）

3. 找到最好的参数

判别模型：逻辑斯蒂回归

步骤：

1. 确定一个函数集

   $$
   P(C_1|x) = {P(x|C_1)P(C_1) \over P(x|C_1)P(C_1)+P(x|C_2)P(C_2)} = {1 \over 1+{P(x|C_2)P(C_2) \over P(x|C_1)P(C_1)}} = {1 \over 1 + e^{-z}}
   $$

   $$
   z = ln{P(x|C_1)P(C_1) \over P(x|C_2)P(C_2)}
   $$

   当 $C_1$ 和 $C_2$ 共用一个协方差矩阵时，经过数学推导，$z = w^Tx + b$

   因此，$f_{w,b}(x)=σ(∑w_ix_i+b)$

2. 评价函数的好坏

   假设训练数据是基于$f_{w,b}(x)=P_{w,b}(C_1|x)$生成的

   给定一组w和b，生成训练数据的概率就是$L(w,b) = f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))...f_{w,b}(x^N)$

   根据最大似然估计，$w^*,b^*=argmaxL(w,b)$

   将其转化成最小化问题，$w*,b*=argmin-lnL(w,b)$（取对数可以让相乘转化成相加）

   进一步地，$-lnL(w,b)$中的每一项可以统一写成$-[ŷlnf(x^n)+(1-ŷ)ln(1-f(x^n))]$

   这样就得到了交叉熵损失函数$L(f)=H(f(x),ŷ)=∑-[ŷlnf(x^n)+(1-ŷ)ln(1-f(x^n))]$

3. 寻找最好的函数

   梯度下降 $w_i←w_i-η∑-(ŷ-f_{w,b}(x^n))x^n_i$

局限性：逻辑斯蒂回归的决策边界是直线，如果两个类别的样本点在特征空间中不能用一条直线分开，它就做不了。解决办法是做特征变换，使得样本点在另一个特征空间中可以用一条直线分开。但是我们不想人为地去做特征变换，我们想让机器自己去做，这也是可以办得到的。可以让原特征先通过一层逻辑斯蒂回归，得到一组新的特征，且这组特征能够让样本点用一条直线分开，最后再做一次逻辑斯蒂回归来分类。

思考：

1. 分类能用回归的方法来硬做吗？为什么？

2. 分类任务损失函数如何定义？

   在训练集上分类错误的次数？但是这样的函数不可微分

3. 为什么分类中的损失函数不能用平方误差？

## 机器学习任务策略

先观察训练集的loss

如果训练集的loss大，则可能是model bias或optimization的问题。如何判断究竟是哪一个问题？用一些较浅的网络或较简单的模型作比较，比较两者在训练集上的loss，如果较深的网络的loss还比较浅的网络的loss要大，说明是optimization没有做好。因为简单模型能做到的事情，复杂模型一定能做到

如果训练集的loss小，再看测试集的loss。如果测试集的loss小，那就结束了。如果测试集的loss大，则有可能是过拟合。

## 局部最小值(local minima)和鞍点(saddle point)

实际中local minima很少见，当gradient很小时，基本都是saddle point

## 批次(batch)和动量(momentum)

在optimization的实际操作中，在求Gradient的时候，如果手里有N个数据，一般会把这N个数据分成一个一个的Batch，每次只拿一个Batch里的数据来算 Loss，根据这个Loss来算Gradient，用这个Gradient来更新参数，这个过程不断进行。把所有的Batch都做过一次，叫做一个Epoch，每一次更新参数叫做一次Update

small batch和large batch的优缺点：

1. epoch，small batch长，large batch短
2. gradient，small batch比较noise，large batch比较stable
3. optimization，small batch更好
4. generalization，small batch更好

momentum能使得gradient descent具有一定的惯性。具体实现原理是gradient decent过程中不再只考虑当前点的gradient，还要考虑前一步的 update，即每次update的量是$momentum × last\_update - learning\_rate × gradient$。对这一改进的另外一个理解是，每次update考虑的是历次梯度加权求和的结果

batch和momentum都能帮助我们跳出saddle point和local minima

## 自动调整学习率(adaptive learning rate)

我们希望学习率是可调整的，在梯度大的地方学习率小，在梯度小的地方学习率大

这个可以通过将η(learning rate)除以一个随update周期变化的参数来做到，典型算法有Adagrad和RMSProp

目前最先进的optimization算法为Adam(RMSProp + Momentum)

learning rate decay：随着训练的进行，当我们越来越接近终点的时候，降低学习率

## 卷积神经网络(CNN)

在影像辨识中，辨识一张图片是不是某一个目标往往是通过观察其中有没有出现某些相关图案来实现的。这就意味着，不需要把整一张图片都输入给每个神经元，即不需要fully connected，只需要让每个神经元负责一定的图片区域就可以了

在CNN中，我们会设定一个区域叫做感受野(Receptive Field)，每一个神经元都只考虑自己的感受野就好了。例如，某一个神经元考虑某一个3×3×3的区域，就将这个区域拉直成一个27维的向量，作为这个神经元的输入。请注意，感受野之间可以重叠，多个神经元可以去守备同一个感受野

权值共享：同样是鸟嘴，可以出现在图片的不同位置上。假设相应的感受野都有一个负责侦测鸟嘴的神经元去守备，那么这些神经元之间干的事情其实是一样的，都是去侦测鸟嘴，那我们就可以让这些神经元的共享参数

Typical Setting

1. 考虑 all channels
2. kernel size 为3×3
3. stride 为1或2
4. 超出范围就 padding (补0)
5. 从 一个 Receptive Field 会有一组 Neural 去守备它，不同 Receptive Field 的 Neural 的参数一一对应共享，Neural 的参数叫做 filter。共用参数这个事情，其实就是用一个 filter 扫过整张图片，这个过程就是 convolution
6. 卷积层叠得越深，看的范围越来越大，不怕侦测不到大的 pattern

![image-20230313093726368](https://raw.githubusercontent.com/Cukoo-Li/typora-images/main/2023/03/upgit_20230313_1678671448.png)

pooling：把图片变小（下采样），减小运算量 

CNN 不能解决影像放大、缩小和旋转的问题，我们需要进行数据增强

## Spatial Transformer Layer

能对输入图像进行旋转、缩放、平移等空间变换，有点像数据增强

## 自注意力机制 (Self - attention)

